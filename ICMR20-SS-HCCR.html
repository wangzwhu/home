<<<<<<< HEAD
<div id="doc" class="markdown-body container-fluid comment-enabled" data-hard-breaks="true" style="position: relative;"><img src="https://i.imgur.com/KBjISso.png" height="200" width="600"><h1 id="ICMR-2020-Call-for-special-session-papers" style=""><a href="http://icmr2020.org/index.html" target="_blank" rel="noopener">ICMR 2020</a>: <em>Call for special session papers</em></h1><h2 id="Human-Centric-Cross-modal-Retrieval" style="">Human-Centric Cross-modal Retrieval</h2><p>The world surrounding us involves multi-modality data – we see objects, hear sounds, feel textures, and so on. With the development of digital devices and social networks, the data is growing rapidly. We urgently need advanced techniques on the understanding, translation, and retrieval of the multi-modality data. The human-centric cross-modal retrieval is mainly motivated by three aspects: 1) humans are the main understanding targets, since the retrieval of human face, appearance, action, behavior is usually a major function of multimedia systems; 2) the evaluation of the retrieval results should essentially rely on human intelligence rather than just machine intelligence, since retrieval models are usually designed by humans and the final decisions would be made by humans; 3) we consider retrieval important not only because of its efficiency and convenience, but also because we use this technology to improve the human life. Hence, we believe that human-centric is the key value of cross-modal retrieval tasks. Maintaining the “human element” will greatly boost the cross-modal retrieval. Human-centric cross-modal retrieval starts to attract increasing research efforts from both academic and industrial research communities.</p><p>Human-centric cross-modal retrieval aims to take one arbitrary type of data as the query to retrieve relevant data of another type, where 1) the retrieval target is focused on “human”, 2) the system collaborates with human intelligence, or 3) the results benefit human life. The challenges of the task mainly lie in the heterogeneity gap which focuses on measuring the content similarity between different modalities of data, the relations of human intelligence and machine intelligence and the ambiguous understanding among different persons. Recently, great advancements in machine learning and artificial intelligence with deep neural networks have made human-centric cross-modal retrieval possible.</p><p>This <a href="http://icmr2020.org/index.html" target="_blank" rel="noopener">ICMR’20</a> special session aims to gather high-quality contributions reporting the most recent progress on human-centric cross-modal retrieval for multimedia applications. It targets a mixed audience of researchers and technologists from several communities, <em>i.e.</em>, multimedia, machine learning, computer vision, artificial intelligence, <em>etc</em>. The topics of interest include, but are not limited to:</p><ul>
=======
<div id="doc" class="markdown-body container-fluid comment-enabled" data-hard-breaks="true" style="position: relative;"><img src="https://i.imgur.com/KBjISso.png" height="200" width="600"><h2 id="ICMR-2020-Call-for-special-session-papers"><a href="http://icmr2020.org/index.html" target="_blank" rel="noopener">ICMR 2020</a>: <em>Call for special session papers</em></h2><h2 id="Human-Centric-Cross-modal-Retrieval">Human-Centric Cross-modal Retrieval</h2><p>The world surrounding us involves multi-modality data – we see objects, hear sounds, feel textures, and so on. With the development of digital devices and social networks, the data is growing rapidly. We urgently need advanced techniques on the understanding, translation, and retrieval of the multi-modality data. The human-centric cross-modal retrieval is mainly motivated by three aspects: 1) humans are the main understanding targets, since the retrieval of human face, appearance, action, behavior is usually a major function of multimedia systems; 2) the evaluation of the retrieval results should essentially rely on human intelligence rather than just machine intelligence, since retrieval models are usually designed by humans and the final decisions would be made by humans; 3) we consider retrieval important not only because of its efficiency and convenience, but also because we use this technology to improve the human life. Hence, we believe that human-centric is the key value of cross-modal retrieval tasks. Maintaining the “human element” will greatly boost the cross-modal retrieval. Human-centric cross-modal retrieval starts to attract increasing research efforts from both academic and industrial research communities.</p><p>Human-centric cross-modal retrieval aims to take one arbitrary type of data as the query to retrieve relevant data of another type, where 1) the retrieval target is focused on “human”, 2) the system collaborates with human intelligence, or 3) the results benefit human life. The challenges of the task mainly lie in the heterogeneity gap which focuses on measuring the content similarity between different modalities of data, the relations of human intelligence and machine intelligence and the ambiguous understanding among different persons. Recently, great advancements in machine learning and artificial intelligence with deep neural networks have made human-centric cross-modal retrieval possible.</p><p>This <a href="http://icmr2020.org/index.html" target="_blank" rel="noopener">ICMR’20</a> special session aims to gather high-quality contributions reporting the most recent progress on human-centric cross-modal retrieval for multimedia applications. It targets a mixed audience of researchers and technologists from several communities, <em>i.e.</em>, multimedia, machine learning, computer vision, artificial intelligence, <em>etc</em>. The topics of interest include, but are not limited to:</p><ul>
>>>>>>> d4dc6b01a28b91f7917c1e34c4f595725a328a95
<li>Cross-modality Face Recognition</li>
<li>Cross-modality Person Re-identification</li>
<li>Cross-modality Hashing</li>
<li>Content-Based Video-Music Retrieval</li>
<li>Sketch-based Manga/Product/Person/Face Retrieval</li>
<li>Cross-modal Scene Recognition</li>
<li>Cross-modal Recipe Retrieval</li>
<li>Video/Image and Language Representation and Retrieval</li>
<li>Weakly-supervised/Unsupervised Learning for Cross-modal Retrieval</li>
<li>Deep learning and Reinforcement Learning for Cross-modal Retrieval</li>
<li>Domain Adaptation for Cross-modal Retrieval</li>
<li>Personalized Cross-modal Retrieval</li>
<li>Cross-modal Pre-training for Multimedia Application</li>
<li>Benchmark Dataset and Performance Evaluation for Cross-modal Retrieval</li>
</ul><h3 id="Maximum-Length-of-a-Paper">Maximum Length of a Paper</h3><p>Each full paper should be limited to <strong>6-8 pages (6 pages limit + references)</strong>.</p><h3 id="Important-Dates">Important Dates</h3><p>Paper Submission: <strong>January 11, 2020</strong><br>
Notification of Acceptance: <strong>March 15, 2020</strong><br>
<<<<<<< HEAD
Camera-Ready Papers Due: <strong>March 31, 2020</strong></p><h3 id="Submission-Instructions" style="">Submission Instructions</h3><p>See the <a href="http://icmr2020.org/authors.html#submit" target="_blank" rel="noopener">ICMR 2020 Paper submission section</a>.</p><h3 id="Organizers" style="">Organizers</h3><table>
<tbody><tr>
<td>
<div style="vertical-align: middle;display: table-cell;">
<div><img src="https://i.imgur.com/xQoeRXU.jpg" height="200"></div>
<div>Zheng Wang, NII<br>
<a href="mailto:wangz@nii.ac.jp" target="_blank" rel="noopener"><span>wangz@nii.ac.jp</span>
</a>
</div>
</div>
</td>
<td>
<div style="vertical-align: middle;display: table-cell;">
<div><img src="https://i.imgur.com/NtAzxQf.jpg" height="200"></div>
<div>Mang Ye, IIAI<br>
<a href="mailto:mangye16@gmail.com" target="_blank" rel="noopener"><span>mangye16@gmail.com</span></a>
</div>
</div>
</td>
<td>
<div style="vertical-align: middle;display: table-cell;">
<div><img src="https://i.imgur.com/Rk7uYCa.jpg" height="200">
</div>
<div>Wenjun Zeng, Microsoft<br>
<a href="mailto:wezeng@microsoft.com" target="_blank" rel="noopener"><span>wezeng@microsoft.com</span>
</a>
</div>
</div>
</td><td>
<div style="vertical-align: middle;display: table-cell;">
<div><img src="https://i.imgur.com/hj32uk0.jpg" height="200">
</div>
<div>Shin'ichi Satoh, NII/UTokyo<br>
<a href="mailto:satoh@nii.ac.jp" target="_blank" rel="noopener"><span>satoh@nii.ac.jp</span>
</a>
</div>
</div>
</td>
</tr>
</tbody></table></div>
=======
Camera-Ready Papers Due: <strong>March 31, 2020</strong></p><h3 id="Submission-Instructions">Submission Instructions</h3><p>See the <a href="http://icmr2020.org/authors.html#submit" target="_blank" rel="noopener">ICMR 2020 Paper submission section</a>.</p><h3 id="Organizers">Organizers</h3><div class="col-xs-6 col-sm-4 col-lg-3 oc-cell">
<div class="row"><img class="oc-photo" src="https://i.imgur.com/xQoeRXU.jpg" height="200"></div>
<div class="row oc-name">Zheng Wang, NII
<br>
<a href="mailto:wangz@nii.ac.jp" target="_blank" rel="noopener">
<span class="oc-email">wangz@nii.ac.jp</span>
</a>
</div>
</div><div class="col-xs-6 col-sm-4 col-lg-3 oc-cell">
<div class="row"><img class="oc-photo" src="https://i.imgur.com/NtAzxQf.jpg" height="200"></div>
<div class="row oc-name">Mang Ye, IIAI
<br>
<a href="mailto:mangye16@gmail.com" target="_blank" rel="noopener"><span class="oc-email">mangye16@gmail.com</span></a>
</div>
</div><div class="col-xs-6 col-sm-4 col-lg-3 oc-cell">
<div class="row">
<img class="oc-photo" src="https://i.imgur.com/Rk7uYCa.jpg" height="200">
</div>
<div class="row oc-name">Wenjun Zeng, Microsoft<br>
<a href="mailto:wezeng@microsoft.com" target="_blank" rel="noopener">
<span class="oc-email">wezeng@microsoft.com</span>
</a>
</div>
</div><div class="col-xs-6 col-sm-4 col-lg-3 oc-cell">
<div class="row">
<img class="oc-photo" src="https://i.imgur.com/hj32uk0.jpg" height="200">
</div>
<div class="row oc-name">Shin'ichi Satoh, NII<br>
<a href="mailto:satoh@nii.ac.jp" target="_blank" rel="noopener">
<span class="oc-email">satoh@nii.ac.jp</span>
</a>
</div>
</div></div>
>>>>>>> d4dc6b01a28b91f7917c1e34c4f595725a328a95
