<div id="doc" class="markdown-body container-fluid comment-enabled" data-hard-breaks="true" style="position: relative;"><img src="https://i.imgur.com/7JDxDMx.gif"><h1 id="ICME-2020-Call-for-special-session-papers" style=""><a href="http://www.2020.ieeeicme.org/" target="_blank" rel="noopener">ICME 2020</a>: <em>Call for special session papers</em></h1><h2 id="Graph-Neural-Networks-for-Multimedia-Representation-Learning" style="">Graph Neural Networks for Multimedia Representation Learning</h2><p>Graphs are flexible mathematical structures modeling pairwise relations between data entities, such as brain networks, social networks, transportation networks, knowledge graphs, irregular geometric data, <em>etc</em>. Graph Neural Networks (GNNs) are generalizations of CNNs to graph-structured data, which have been attracting increasing attention due to the great expressive power. Thanks to its convincing performance and high interpretability, GNNs have been widely employed to represent various kinds of multimedia data, such as social networks, 3D point clouds, natural languages, <em>etc</em>.</p><p>While GNNs have progressed in multimedia representation learning, GNN models still remain challenging in offering satisfactory representation for multimedia data theoretically, <em>e.g.</em>, optimizing the underlying graph, designing deep GNN representation, addressing dynamic graph structures, <em>etc</em>. Moreover, how to develop scalable GNNs for large-scale multimedia data still requires many efforts.</p><p>This special session serves as a forum for researchers from academia and industry to share their insights and cutting-edge results in GNNs for multimedia representation learning. Both theoretical development and applications of GNNs are welcome for submission.</p><p>Topics of particular interest include, but are not limited to:</p><ul>
<li>Graph learning in GNNs for Multimedia Representation</li>
<li>GNNs for Deep Multimedia Representation Learning</li>
<li>GNNs for Time-varying Multimedia Representation Learning</li>
<li>Scalable GNNs for Large-scale Multimedia Data</li>
<li>Directed GNNs for Asymmetric Multimedia Representation</li>
<li>Generative GNN models for Multimedia Data</li>
<li>GNNs for Innovative Multimedia Applications, such as Person Re-identification, Social Network Analysis, Point Cloud processing, Image Retrieval, Recommendation Systems and so on.</li>
</ul><h3 id="Maximum-Length-of-a-Paper" style="">Maximum Length of a Paper</h3><p>Papers must be no longer than 6 pages, including all text, figures, and references.</p><h3 id="Important-Dates" style="">Important Dates</h3><p>Paper Submission: <strong>29 Nov 2019</strong><br>
Notification of Acceptance: <strong>6 Mar 2020</strong><br>
Camera-Ready Papers Due: <strong>3 Apr 2020</strong></p><h3 id="Submission-Instructions" style="">Submission Instructions</h3><p>See the <a href="http://www.2020.ieeeicme.org/index.php/author-information-and-submission-instructions/#Regular" target="_blank" rel="noopener">ICME 2020 Paper submission section</a>.</p><h3 id="Organizers" style="">Organizers</h3><table>
<tbody><tr>
<td>
<div style="vertical-align: middle;display: table-cell;">
<div><img src="https://i.imgur.com/n0nq7Rh.jpg" height="200"></div>
<div>Wei Hu, PKU<br>
<a href="mailto:forhuwei@pku.edu.cn" target="_blank" rel="noopener"><span>forhuwei@pku.edu.cn</span></a>
</div>
</div>
</td>
<td>
<div style="vertical-align: middle;display: table-cell;">
<div><img src="https://i.imgur.com/xQoeRXU.jpg" height="200"></div>
<div>Zheng Wang, NII<br>
<a href="mailto:wangz@nii.ac.jp" target="_blank" rel="noopener"><span>wangz@nii.ac.jp</span>
</a>
</div>
</div>
</td>
<td>
<div style="vertical-align: middle;display: table-cell;">
<div><img src="https://i.imgur.com/hj32uk0.jpg" height="200">
</div>
<div>Shin'ichi Satoh, NII/UTokyo<br>
<a href="mailto:satoh@nii.ac.jp" target="_blank" rel="noopener"><span>satoh@nii.ac.jp</span>
</a>
</div>
</div>
</td>
<td>
<div style="vertical-align: middle;display: table-cell;">
<div><img src="https://i.imgur.com/B2HKGOQ.jpg" height="200">
</div>
<div>Chia-Wen Lin, NTHU<br>
<a href="mailto:cwlin@ee.nthu.edu.tw" target="_blank" rel="noopener"><span>cwlin@ee.nthu.edu.tw</span>
</a>
</div>
</div>
</td></tr>
</tbody></table></div>